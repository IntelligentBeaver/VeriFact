{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# PubMed Adapter (Entrez API)\n",
    "\n",
    "Purpose:\n",
    "- Harvest biomedical literature from PubMed\n",
    "- Normalize records into internal health document schema\n",
    "- Preserve provenance and raw payloads\n",
    "\n",
    "Source: NCBI Entrez (PubMed)"
   ],
   "metadata": {
    "id": "oW-arGWKDzCK"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Dependencies",
   "metadata": {
    "id": "_gVqmDOBEBOb"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "s1nZ2-BeDXYd"
   },
   "source": [
    "import requests\n",
    "import uuid\n",
    "import json\n",
    "import hashlib\n",
    "import xml.etree.ElementTree as ET\n",
    "from datetime import datetime,timezone\n",
    "from pathlib import Path\n",
    "import time\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import List, Dict, Any, Optional, Union, Tuple\n",
    "from queue import Queue"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Storage directory configurations"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "STORAGE_DIR = Path.cwd().parent / \"storage\"\n",
    "STORAGE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "SEED_DIR=STORAGE_DIR / \"seeds\"\n",
    "SEED_DIR"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Helpers:"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Per-(key,email) rate limiter ---\n",
    "class ApiKeyRateLimiter:\n",
    "    def __init__(self, api_key: str, email: str, delay: float = 0.11):\n",
    "        self.api_key = api_key\n",
    "        self.email = email\n",
    "        self.delay = delay\n",
    "        self.lock = threading.Lock()\n",
    "        self.last_ts = 0.0\n",
    "\n",
    "    def call(self, fn, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Calls fn(*args, api_key=self.api_key, email=self.email, **kwargs)\n",
    "        while enforcing per-key delay in a thread-safe way.\n",
    "        \"\"\"\n",
    "        with self.lock:\n",
    "            now = time.time()\n",
    "            wait = max(0.0, self.delay - (now - self.last_ts))\n",
    "            if wait > 0:\n",
    "                time.sleep(wait)\n",
    "            # inject api_key and email into call\n",
    "            kwargs_with_key = dict(kwargs)\n",
    "            kwargs_with_key[\"api_key\"] = self.api_key\n",
    "            kwargs_with_key[\"email\"] = self.email\n",
    "            result = fn(*args, **kwargs_with_key)\n",
    "            self.last_ts = time.time()\n",
    "            return result"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Build a pool from list of (api_key,email) tuples ---\n",
    "def build_api_key_pool(key_email_pairs: List[Tuple[str,str]], delay_per_key: float = 0.11) -> Queue:\n",
    "    q = Queue()\n",
    "    for k, e in key_email_pairs:\n",
    "        q.put(ApiKeyRateLimiter(k, e, delay=delay_per_key))\n",
    "    return q"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Example worker that borrows a limiter, queries PubMed, and returns limiter ---\n",
    "def _search_term_with_key_pool(term: str, retmax: int, key_pool: Queue):\n",
    "    limiter = key_pool.get()  # block until a limiter available\n",
    "    try:\n",
    "        query = build_mesh_aware_query(term)\n",
    "        resp = limiter.call(pubmed_search, query, 0, retmax)  # pubmed_search will receive api_key/email\n",
    "        esr = resp.get(\"esearchresult\", {}) if isinstance(resp, dict) else {}\n",
    "        return {\n",
    "            \"term\": term,\n",
    "            \"query\": query,\n",
    "            \"pmid_count\": int(esr.get(\"count\", \"0\") or 0),\n",
    "            \"pmids\": esr.get(\"idlist\", []),\n",
    "            \"api_key_used\": limiter.api_key[-6:],  # log partial key only\n",
    "            \"email_used\": limiter.email,\n",
    "            \"retrieved_at\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())\n",
    "        }\n",
    "    finally:\n",
    "        key_pool.put(limiter)  # put it back for reuse"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def make_document_id(source: str, source_id: str) -> str:\n",
    "    base = f\"{source}:{source_id}\"\n",
    "    return hashlib.sha256(base.encode()).hexdigest()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Adapter Configuration",
   "metadata": {
    "id": "jl2NcYhpEKLn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ENTREZ_BASE = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/\"\n",
    "EMAIL = \"abamsheikh@gmail.com\"   # REQUIRED by NCBI\n",
    "TOOL = \"health-harvester\"\n",
    "API_KEY = 'ec74621abe110994f710510d05aa0780d607'  # optional but recommended\n",
    "\n",
    "REQUEST_DELAY = 0.1  # seconds (NCBI rate limit safety)"
   ],
   "metadata": {
    "id": "clUcAdyuEJgz"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## ESearch – find PubMed IDs (PMIDs)",
   "metadata": {
    "id": "2oFkFP2eEUEV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# --- Adjusted pubmed_search to accept api_key and email ---\n",
    "def pubmed_search(query: str, retstart=0, retmax=20, api_key: str = None, email: str = None):\n",
    "    params = {\n",
    "        \"db\": \"pubmed\",\n",
    "        \"term\": query,\n",
    "        \"retmode\": \"json\",\n",
    "        \"retstart\": retstart,\n",
    "        \"retmax\": retmax,\n",
    "        \"tool\": TOOL,\n",
    "    }\n",
    "    # allow per-call email override (falls back to global EMAIL)\n",
    "    if email:\n",
    "        params[\"email\"] = email\n",
    "    else:\n",
    "        params[\"email\"] = EMAIL\n",
    "\n",
    "    if api_key:\n",
    "        params[\"api_key\"] = api_key\n",
    "\n",
    "    r = requests.get(f\"{ENTREZ_BASE}/esearch.fcgi\", params=params, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "\n",
    "# def pubmed_search(query: str, retstart=0, retmax=20):\n",
    "#     params = {\n",
    "#         # using Pubmed Database\n",
    "#         \"db\": \"pubmed\",\n",
    "#         # The term used to search for the record\n",
    "#         \"term\": query,\n",
    "#         \"retmode\": \"json\",\n",
    "#         # Sequential index of the first record to be retrieved (default is 0)\n",
    "#         \"retstart\": retstart,\n",
    "#         # Total number of records from the input set to be retrieved, up to a maximum of 10,000.\n",
    "#         \"retmax\": retmax,\n",
    "#         \"email\": EMAIL,\n",
    "#         \"tool\": TOOL,\n",
    "#     }\n",
    "#     if API_KEY:\n",
    "#         params[\"api_key\"] = API_KEY\n",
    "#\n",
    "#     r = requests.get(f\"{ENTREZ_BASE}/esearch.fcgi\", params=params)\n",
    "#     r.raise_for_status()\n",
    "#     return r.json()"
   ],
   "metadata": {
    "id": "d8LsQwSIETr0"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Expected Output:\n",
    "```json\n",
    "\"idlist\" [\"38712345\", \"38699811\", ...]\n",
    "```"
   ],
   "metadata": {
    "id": "p2cEPj9YEd20"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## EFetch – fetch full article records",
   "metadata": {
    "id": "74venhOtEuKs"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def pubmed_fetch(pmids: List[str]) -> str:\n",
    "    params = {\n",
    "        \"db\": \"pubmed\",\n",
    "        \"id\": \",\".join(pmids),\n",
    "        \"retmode\": \"xml\",\n",
    "        # Rettype is null for pubmed database with json retmode\n",
    "        # \"rettype\":null\n",
    "        \"email\": EMAIL,\n",
    "        \"tool\": TOOL,\n",
    "    }\n",
    "    if API_KEY:\n",
    "        params[\"api_key\"] = API_KEY\n",
    "\n",
    "    r = requests.get(f\"{ENTREZ_BASE}/efetch.fcgi\", params=params)\n",
    "    r.raise_for_status()\n",
    "    return r.text"
   ],
   "metadata": {
    "id": "76iYUumkEtsH"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Usage:",
   "metadata": {
    "id": "R6bNF1sCEz-i"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# pmids = search_resp[\"esearchresult\"][\"idlist\"]\n",
    "# xml_data = pubmed_fetch(pmids)\n",
    "# xml_data"
   ],
   "metadata": {
    "id": "D6LrXvZuEy2D"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Parse PubMed XML (core logic)",
   "metadata": {
    "id": "XcQ_HM7KE5KV"
   }
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def _text_or_none(elem: Optional[ET.Element]) -> Optional[str]:\n",
    "    if elem is None or elem.text is None:\n",
    "        return None\n",
    "    return elem.text.strip()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def _parse_pubdate(pubdate_elem: Optional[ET.Element]) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Try to build an ISO-ish date string from PubDate elements.\n",
    "    Handles:\n",
    "      <PubDate><Year>YYYY</Year><Month>Mon</Month><Day>DD</Day></PubDate>\n",
    "      <PubDate><MedlineDate>YYYY MMM</MedlineDate></PubDate>\n",
    "    Returns YYYY-MM-DD if day/month present, otherwise YYYY-MM or YYYY.\n",
    "    \"\"\"\n",
    "    if pubdate_elem is None:\n",
    "        return None\n",
    "\n",
    "    # Prefer explicit Year/Month/Day children\n",
    "    year = _text_or_none(pubdate_elem.find(\"Year\"))\n",
    "    month = _text_or_none(pubdate_elem.find(\"Month\"))\n",
    "    day = _text_or_none(pubdate_elem.find(\"Day\"))\n",
    "    medline = _text_or_none(pubdate_elem.find(\"MedlineDate\"))\n",
    "\n",
    "    if year:\n",
    "        # Try to convert a month name to numeric if present\n",
    "        month_num = None\n",
    "        if month:\n",
    "            try:\n",
    "                # Accept month abbreviations like \"Jun\" or numeric strings\n",
    "                month_num = datetime.strptime(month[:3], \"%b\").month\n",
    "            except Exception:\n",
    "                # fallback: if month is numeric already\n",
    "                try:\n",
    "                    month_num = int(month)\n",
    "                except Exception:\n",
    "                    month_num = None\n",
    "\n",
    "        # Construct ISO-like string\n",
    "        if month_num and day:\n",
    "            try:\n",
    "                return f\"{int(year):04d}-{int(month_num):02d}-{int(day):02d}\"\n",
    "            except Exception:\n",
    "                return f\"{year}-{month_num}-{day}\"\n",
    "        if month_num:\n",
    "            return f\"{int(year):04d}-{int(month_num):02d}\"\n",
    "        return f\"{int(year):04d}\"\n",
    "\n",
    "    if medline:\n",
    "        # medline often contains \"2001 Jun\" or ranges; return raw medline as fallback\n",
    "        return medline\n",
    "\n",
    "    return None"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def parse_pubmed_xml(xml_str: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Parse PubMed EFetch XML and return a list of record dicts with expanded fields.\n",
    "    Safe if some nodes are missing.\n",
    "    \"\"\"\n",
    "    root = ET.fromstring(xml_str)\n",
    "    records: List[Dict] = []\n",
    "\n",
    "    for article in root.findall(\".//PubmedArticle\"):\n",
    "        rec: Dict = {}\n",
    "\n",
    "        # PMIDs: prefer MedlineCitation/PMID but also capture ArticleIdList if present\n",
    "        pmid = _text_or_none(article.find(\".//MedlineCitation/PMID\"))\n",
    "        rec[\"pmid\"] = pmid\n",
    "\n",
    "        # Title\n",
    "        rec[\"title\"] = _text_or_none(article.find(\".//Article/ArticleTitle\"))\n",
    "\n",
    "        # Abstract: concatenate AbstractText parts; keep labels if present\n",
    "        abstract_elems = article.findall(\".//Article/Abstract/AbstractText\")\n",
    "        if abstract_elems:\n",
    "            parts = []\n",
    "            for p in abstract_elems:\n",
    "                label = p.get(\"Label\") or p.get(\"NlmCategory\")\n",
    "                text = _text_or_none(p)\n",
    "                if text:\n",
    "                    if label:\n",
    "                        parts.append(f\"{label}: {text}\")\n",
    "                    else:\n",
    "                        parts.append(text)\n",
    "            rec[\"abstract\"] = \" \".join(parts).strip() if parts else None\n",
    "        else:\n",
    "            rec[\"abstract\"] = None\n",
    "\n",
    "        # Copyright info (if present under Abstract)\n",
    "        rec[\"copyright\"] = _text_or_none(article.find(\".//Article/Abstract/AbstractText/..//CopyrightInformation\")) \\\n",
    "            or _text_or_none(article.find(\".//Article/Abstract/CopyrightInformation\"))\n",
    "\n",
    "        # Journal fields\n",
    "        journal_title = _text_or_none(article.find(\".//Article/Journal/Title\"))\n",
    "        iso_abbrev = _text_or_none(article.find(\".//Article/Journal/ISOAbbreviation\"))\n",
    "        issn_print = _text_or_none(article.find(\".//Article/Journal/ISSN\"))\n",
    "        # fallback linking ISSN in MedlineJournalInfo\n",
    "        issn_linking = _text_or_none(article.find(\".//MedlineJournalInfo/ISSNLinking\"))\n",
    "\n",
    "        rec[\"journal\"] = journal_title\n",
    "        rec[\"journal_isoabbrev\"] = iso_abbrev\n",
    "        rec[\"journal_issn\"] = issn_print or issn_linking\n",
    "\n",
    "        # Pagination and pages\n",
    "        start_page = _text_or_none(article.find(\".//Pagination/StartPage\"))\n",
    "        end_page = _text_or_none(article.find(\".//Pagination/EndPage\"))\n",
    "        medline_pgn = _text_or_none(article.find(\".//Pagination/MedlinePgn\"))\n",
    "        # fallback to MedlineCitation/Article/Journal/JournalIssue\n",
    "        if not start_page and not end_page:\n",
    "            # some records have MedlinePgn at MedlineCitation level\n",
    "            medline_pgn = medline_pgn or _text_or_none(article.find(\".//MedlineCitation/Article/Pagination/MedlinePgn\"))\n",
    "\n",
    "        rec[\"start_page\"] = start_page\n",
    "        rec[\"end_page\"] = end_page\n",
    "        rec[\"medline_pgn\"] = medline_pgn\n",
    "\n",
    "        # Volume / Issue\n",
    "        rec[\"volume\"] = _text_or_none(article.find(\".//Article/Journal/JournalIssue/Volume\"))\n",
    "        rec[\"issue\"] = _text_or_none(article.find(\".//Article/Journal/JournalIssue/Issue\"))\n",
    "\n",
    "        # Publication date (Article/Journal/JournalIssue/PubDate) and history dates\n",
    "        pubdate_elem = article.find(\".//Article/Journal/JournalIssue/PubDate\")\n",
    "        rec[\"pub_date\"] = _parse_pubdate(pubdate_elem)\n",
    "        # DateCompleted / DateRevised under MedlineCitation\n",
    "        rec[\"date_completed\"] = _parse_pubdate(article.find(\".//MedlineCitation/DateCompleted\"))\n",
    "        rec[\"date_revised\"] = _parse_pubdate(article.find(\".//MedlineCitation/DateRevised\"))\n",
    "\n",
    "        # PubMed history (multiple PubMedPubDate entries)\n",
    "        pubmed_dates = []\n",
    "        for d in article.findall(\".//PubmedData/History/PubMedPubDate\"):\n",
    "            status = d.get(\"PubStatus\")\n",
    "            y = _text_or_none(d.find(\"Year\"))\n",
    "            m = _text_or_none(d.find(\"Month\"))\n",
    "            day = _text_or_none(d.find(\"Day\"))\n",
    "            if y:\n",
    "                # try to form YYYY-MM-DD if possible\n",
    "                try:\n",
    "                    month_num = int(m) if m and m.isdigit() else None\n",
    "                except Exception:\n",
    "                    month_num = None\n",
    "                if month_num and day:\n",
    "                    value = f\"{int(y):04d}-{int(month_num):02d}-{int(day):02d}\"\n",
    "                elif month_num:\n",
    "                    value = f\"{int(y):04d}-{int(month_num):02d}\"\n",
    "                else:\n",
    "                    value = f\"{int(y):04d}\"\n",
    "            else:\n",
    "                value = None\n",
    "            pubmed_dates.append({\"status\": status, \"value\": value})\n",
    "        rec[\"pubmed_history\"] = pubmed_dates\n",
    "\n",
    "        # Language\n",
    "        rec[\"language\"] = _text_or_none(article.find(\".//Article/Language\")) or _text_or_none(article.find(\".//MedlineCitation/Language\"))\n",
    "\n",
    "        # Publication types\n",
    "        pub_types = [ _text_or_none(pt) for pt in article.findall(\".//Article/PublicationTypeList/PublicationType\") ]\n",
    "        pub_types = [p for p in pub_types if p]\n",
    "        rec[\"publication_types\"] = pub_types\n",
    "\n",
    "        # Authors + affiliations\n",
    "        authors_list = []\n",
    "        for a in article.findall(\".//Article/AuthorList/Author\"):\n",
    "            last = _text_or_none(a.find(\"LastName\"))\n",
    "            fore = _text_or_none(a.find(\"ForeName\"))\n",
    "            initials = _text_or_none(a.find(\"Initials\"))\n",
    "            affiliation = None\n",
    "            affs = []\n",
    "            # AffiliationInfo may be present multiple times\n",
    "            for aff in a.findall(\"AffiliationInfo\"):\n",
    "                aff_text = _text_or_none(aff.find(\"Affiliation\"))\n",
    "                if aff_text:\n",
    "                    affs.append(aff_text)\n",
    "            # some authors may not have LastName/ForeName (corporate authors)\n",
    "            name = None\n",
    "            if last and fore:\n",
    "                name = f\"{fore} {last}\"\n",
    "            elif last:\n",
    "                name = last\n",
    "            elif fore:\n",
    "                name = fore\n",
    "            else:\n",
    "                # check for CollectiveName\n",
    "                coll = _text_or_none(a.find(\"CollectiveName\"))\n",
    "                name = coll\n",
    "\n",
    "            author_entry = {\n",
    "                \"name\": name,\n",
    "                \"given_names\": fore,\n",
    "                \"family_name\": last,\n",
    "                \"initials\": initials,\n",
    "                \"affiliations\": affs,\n",
    "                \"orcid\": None,\n",
    "                \"email\": None,\n",
    "                \"contribution_role\": [],\n",
    "                \"author_id\": None\n",
    "            }\n",
    "            authors_list.append(author_entry)\n",
    "        rec[\"authors\"] = authors_list\n",
    "\n",
    "        # MeSH Headings\n",
    "        mesh_terms = []\n",
    "        for mh in article.findall(\".//MeshHeadingList/MeshHeading\"):\n",
    "            descriptor = _text_or_none(mh.find(\"DescriptorName\"))\n",
    "            qualifiers = [ _text_or_none(q) for q in mh.findall(\"QualifierName\") ]\n",
    "            qualifiers = [q for q in qualifiers if q]\n",
    "            if descriptor:\n",
    "                mesh_terms.append({\"descriptor\": descriptor, \"qualifiers\": qualifiers})\n",
    "        rec[\"mesh_headings\"] = mesh_terms\n",
    "\n",
    "        # References / ArticleIdList (pubmed, doi, pii, pmc etc.)\n",
    "        article_ids = []\n",
    "        for aid in article.findall(\".//PubmedData/ArticleIdList/ArticleId\"):\n",
    "            idtype = aid.get(\"IdType\")\n",
    "            val = _text_or_none(aid)\n",
    "            if idtype and val:\n",
    "                article_ids.append({\"type\": idtype.lower(), \"value\": val})\n",
    "        rec[\"article_ids\"] = article_ids\n",
    "\n",
    "        # If DOI present also put it at top-level 'doi' convenience key\n",
    "        doi_val = None\n",
    "        for a in article_ids:\n",
    "            if a[\"type\"] == \"doi\":\n",
    "                doi_val = a[\"value\"]\n",
    "                break\n",
    "        rec[\"doi\"] = doi_val\n",
    "\n",
    "        # Collect Mesh term strings as convenience list (descriptor only)\n",
    "        rec[\"mesh_terms_simple\"] = [m[\"descriptor\"] for m in mesh_terms]\n",
    "\n",
    "        # Anything else useful: NLM unique id, MedlineTA (journal short), country\n",
    "        rec[\"nlm_unique_id\"] = _text_or_none(article.find(\".//MedlineJournalInfo/NlmUniqueID\"))\n",
    "        rec[\"medline_ta\"] = _text_or_none(article.find(\".//MedlineJournalInfo/MedlineTA\"))\n",
    "        rec[\"country\"] = _text_or_none(article.find(\".//MedlineJournalInfo/Country\"))\n",
    "\n",
    "        # Append record\n",
    "        records.append(rec)\n",
    "\n",
    "    return records"
   ],
   "metadata": {
    "collapsed": true,
    "id": "IB98gJ0VE7Vt"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Usage:",
   "metadata": {
    "id": "5bHyS0RAE-sQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# parsed = parse_pubmed_xml(xml_data)\n",
    "# parsed[:1]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "04hIcWiSE-D9",
    "outputId": "64eb9da3-3657-43ce-cf6c-16cbabd48acf"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Normalize to internal schema",
   "metadata": {
    "id": "KIwH8AKYH-IQ"
   }
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def _sha256_of_bytes(b: bytes) -> str:\n",
    "    import hashlib\n",
    "    return hashlib.sha256(b).hexdigest()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def normalize_pubmed_record(\n",
    "    rec: Dict[str, Any],\n",
    "    raw_ref: Optional[str] = None,\n",
    "    raw_bytes: Optional[bytes] = None,\n",
    "    fetched_url: Optional[str] = None,\n",
    "    harvester_id: str = \"health-harvester\",\n",
    "    adapter_name: str = \"pubmed_adapter\",\n",
    "    adapter_version: str = \"1.0\"\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Normalize a parsed PubMed record (output of parse_pubmed_xml) to the user's JSON schema.\n",
    "    - rec: dict with keys like pmid, doi, title, abstract, authors (list), article_ids (list of {type,value}), pub_date, volume, issue, medline_pgn, journal_issn, mesh_terms_simple, etc.\n",
    "    - raw_ref: storage reference for raw payload (eg s3://...)\n",
    "    - raw_bytes: optional raw XML bytes (to compute sha256 and size)\n",
    "    - fetched_url: optional URL used to fetch (useful if adapter fetched via efetch url)\n",
    "    \"\"\"\n",
    "    now = datetime.now(timezone.utc).isoformat() + \"Z\"\n",
    "    source = \"pubmed\"\n",
    "    pmid = rec.get(\"pmid\")\n",
    "    source_id = f\"PMID:{pmid}\" if pmid else None\n",
    "\n",
    "    # Build identifiers list (include pmid and any ArticleIdList entries)\n",
    "    identifiers: List[Dict[str,str]] = []\n",
    "    if pmid:\n",
    "        identifiers.append({\"type\": \"pmid\", \"value\": str(pmid)})\n",
    "\n",
    "    # Add article_ids from parse (rec['article_ids'] expected as list of {\"type\",\"value\"})\n",
    "    for aid in rec.get(\"article_ids\", []):\n",
    "        t = aid.get(\"type\")\n",
    "        v = aid.get(\"value\")\n",
    "        if t and v:\n",
    "            # normalize type names\n",
    "            identifiers.append({\"type\": t.lower(), \"value\": v})\n",
    "\n",
    "    # convenience DOI\n",
    "    doi = rec.get(\"doi\")\n",
    "    if doi and not any(i[\"type\"] == \"doi\" for i in identifiers):\n",
    "        identifiers.append({\"type\": \"doi\", \"value\": doi})\n",
    "\n",
    "    # Authors mapping (rec['authors'] expected as list with name, given_names, family_name, affiliations)\n",
    "    authors_out = []\n",
    "    affiliations_agg = []\n",
    "    for a in rec.get(\"authors\", []):\n",
    "        name = a.get(\"name\")\n",
    "        given = a.get(\"given_names\") or a.get(\"given\") or a.get(\"fore\")\n",
    "        family = a.get(\"family_name\") or a.get(\"last\")\n",
    "        affs = a.get(\"affiliations\") or []\n",
    "        # aggregate unique affs\n",
    "        for af in affs:\n",
    "            if af and af not in affiliations_agg:\n",
    "                affiliations_agg.append(af)\n",
    "\n",
    "        author_entry = {\n",
    "            \"name\": name,\n",
    "            \"given_names\": given,\n",
    "            \"family_name\": family,\n",
    "            \"affiliations\": affs,\n",
    "            \"orcid\": a.get(\"orcid\"),\n",
    "            \"email\": a.get(\"email\"),\n",
    "            \"contribution_role\": a.get(\"contribution_role\") or [],\n",
    "            \"author_id\": a.get(\"author_id\") or None\n",
    "        }\n",
    "        authors_out.append(author_entry)\n",
    "\n",
    "    # Text sections: put abstract as a section (start/end offsets not available here; set to 0..len-1)\n",
    "    text_sections = []\n",
    "    abstract_text = rec.get(\"abstract\")\n",
    "    if abstract_text:\n",
    "        start = 0\n",
    "        end = len(abstract_text)\n",
    "        text_sections.append({\n",
    "            \"label\": \"abstract\",\n",
    "            \"text\": abstract_text,\n",
    "            \"start_offset\": start,\n",
    "            \"end_offset\": end\n",
    "        })\n",
    "\n",
    "    # Pages: try to assemble pages field\n",
    "    pages = None\n",
    "    if rec.get(\"medline_pgn\"):\n",
    "        pages = rec.get(\"medline_pgn\")\n",
    "    else:\n",
    "        s = rec.get(\"start_page\")\n",
    "        e = rec.get(\"end_page\")\n",
    "        if s and e:\n",
    "            pages = f\"{s}-{e}\"\n",
    "        elif s:\n",
    "            pages = s\n",
    "\n",
    "    # Access: detect PMC ID -> build PMC url, set has_fulltext True\n",
    "    fulltext_urls = []\n",
    "    has_fulltext = False\n",
    "    for ident in identifiers:\n",
    "        if ident[\"type\"] == \"pmc\" or (ident[\"type\"] == \"pmcid\"):\n",
    "            pmcval = ident[\"value\"]\n",
    "            # normalize pmc id to start with PMC\n",
    "            if not pmcval.upper().startswith(\"PMC\"):\n",
    "                pmcval = \"PMC\" + pmcval\n",
    "            fulltext_urls.append({\"url\": f\"https://www.ncbi.nlm.nih.gov/pmc/articles/{pmcval}/\", \"format\": \"html\", \"source\": \"pmc\"})\n",
    "            has_fulltext = True\n",
    "\n",
    "    # If DOI exists, we may set has_fulltext unknown (we will enrich later via Unpaywall)\n",
    "    access_type = \"unknown\"\n",
    "    if has_fulltext:\n",
    "        access_type = \"open\"\n",
    "\n",
    "    # ingestion raw metadata\n",
    "    raw_size = len(raw_bytes) if raw_bytes is not None else None\n",
    "    raw_sha = _sha256_of_bytes(raw_bytes) if raw_bytes is not None else None\n",
    "\n",
    "    # Determine published_date: prefer rec['pub_date'] else date_completed else pubmed_history's pubmed entry\n",
    "    published_date = rec.get(\"pub_date\") or rec.get(\"date_completed\")\n",
    "    if not published_date:\n",
    "        # try to find PubStatus 'pubmed' or 'medline' in rec['pubmed_history']\n",
    "        for d in rec.get(\"pubmed_history\", []) or []:\n",
    "            if d.get(\"status\") == \"pubmed\" and d.get(\"value\"):\n",
    "                published_date = d.get(\"value\"); break\n",
    "            if d.get(\"status\") == \"medline\" and d.get(\"value\"):\n",
    "                published_date = d.get(\"value\")\n",
    "\n",
    "    # Mesh terms: prefer rec['mesh_terms_simple'] if available, else structured mesh_headings\n",
    "    mesh_terms = []\n",
    "    if rec.get(\"mesh_terms_simple\"):\n",
    "        mesh_terms = rec.get(\"mesh_terms_simple\")\n",
    "    else:\n",
    "        for m in rec.get(\"mesh_headings\", []):\n",
    "            if isinstance(m, dict):\n",
    "                mesh_terms.append(m.get(\"descriptor\"))\n",
    "\n",
    "    # Build journal object\n",
    "    journal_obj = {\n",
    "        \"name\": rec.get(\"journal\"),\n",
    "        \"issn\": rec.get(\"journal_issn\"),\n",
    "        \"publisher\": None,\n",
    "        \"volume\": rec.get(\"volume\"),\n",
    "        \"issue\": rec.get(\"issue\"),\n",
    "        \"pages\": pages\n",
    "    }\n",
    "\n",
    "    # processing dedup key: use DOI if present, else PMID\n",
    "    dedup_key = None\n",
    "    if doi:\n",
    "        dedup_key = doi.lower()\n",
    "    elif pmid:\n",
    "        dedup_key = f\"pmid:{pmid}\"\n",
    "\n",
    "    # put publication types into tags\n",
    "    tags = rec.get(\"publication_types\") or []\n",
    "\n",
    "    document = {\n",
    "        \"schema_version\": \"1.0\",\n",
    "        \"document_id\": make_document_id(source, source_id) if source_id else make_document_id(source, str(doi or uuid.uuid4())),\n",
    "        \"source\": source,\n",
    "        \"source_id\": source_id,\n",
    "        \"identifiers\": identifiers,\n",
    "        \"title\": rec.get(\"title\"),\n",
    "        \"subtitle\": None,\n",
    "        \"authors\": authors_out,\n",
    "        \"affiliations\": [{\"affiliation\": a} for a in affiliations_agg] if affiliations_agg else [],\n",
    "        \"abstract\": abstract_text,\n",
    "        \"plain_text\": abstract_text,\n",
    "        \"text_sections\": text_sections,\n",
    "        \"content_type\": \"journal_article\",\n",
    "        \"language\": rec.get(\"language\") or \"en\",\n",
    "        \"keywords\": rec.get(\"keywords\") or [],\n",
    "        \"mesh_terms\": mesh_terms,\n",
    "        \"topics\": rec.get(\"topics\") or [],\n",
    "        \"published_date\": published_date,\n",
    "        \"journal\": journal_obj,\n",
    "        \"license\": {\n",
    "            \"type\": \"unknown\",\n",
    "            \"url\": None,\n",
    "            \"notes\": None\n",
    "        },\n",
    "        \"access\": {\n",
    "            \"has_fulltext\": bool(has_fulltext),\n",
    "            \"access_type\": access_type,\n",
    "            \"fulltext_urls\": fulltext_urls\n",
    "        },\n",
    "        \"figures\": [],\n",
    "        \"tables\": [],\n",
    "        \"supplementary_files\": [],\n",
    "        \"methods_summary\": None,\n",
    "        \"data_availability\": {},\n",
    "        \"funding\": rec.get(\"funding\") or [],\n",
    "        \"conflicts_of_interest\": rec.get(\"conflicts_of_interest\"),\n",
    "        \"ethics\": {},\n",
    "        \"clinical_trial\": rec.get(\"clinical_trial\") or {},\n",
    "        \"references\": rec.get(\"references\") or [],\n",
    "        \"metrics\": {},\n",
    "        \"ingestion\": {\n",
    "            \"ingested_at\": now,\n",
    "            \"harvester_id\": harvester_id,\n",
    "            \"adapter\": {\n",
    "                \"name\": adapter_name,\n",
    "                \"version\": adapter_version,\n",
    "                \"fetched_at\": now,\n",
    "                \"source_fetch_url\": fetched_url\n",
    "            },\n",
    "            \"raw_payload_ref\": raw_ref,\n",
    "            \"raw_format\": \"xml\",\n",
    "            \"raw_size_bytes\": raw_size,\n",
    "            \"raw_sha256\": raw_sha,\n",
    "            \"source_last_modified\": None\n",
    "        },\n",
    "        \"processing\": {\n",
    "            \"normalized_at\": now,\n",
    "            \"language_detected\": rec.get(\"language\") or \"en\",\n",
    "            \"dedup_key\": dedup_key,\n",
    "            \"canonical_id\": doi.lower() if doi else None,\n",
    "            \"quality_flags\": [],\n",
    "            \"processing_notes\": None\n",
    "        },\n",
    "        \"tags\": tags,\n",
    "        \"security\": {},\n",
    "        \"last_updated\": now\n",
    "    }\n",
    "\n",
    "    return document"
   ],
   "metadata": {
    "id": "owEC-VCWIBQO"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Usage:",
   "metadata": {
    "id": "7j_a4GKRIDYo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# normalized_docs = [\n",
    "#     normalize_pubmed_record(rec, raw_ref=\"pubmed_raw.xml\")\n",
    "#     for rec in parsed\n",
    "# ]\n",
    "# normalized_docs[0]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aRW0qOrLIFni",
    "outputId": "a81d01b1-ea0c-4f21-f5c3-9401c2056935"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Save raw + normalized output",
   "metadata": {
    "id": "HHGNr1MEIXOI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def save_normalized_pubmed(normalized_docs, xml_data=None, storage_dir: Path = STORAGE_DIR) -> dict:\n",
    "    \"\"\"\n",
    "    Saves the raw XML and normalized JSON of PubMed data.\n",
    "\n",
    "    Parameters:\n",
    "    - normalized_docs: list/dict of normalized documents\n",
    "    - xml_data: optional raw XML string to save\n",
    "    - storage_dir: Path object for where to save files\n",
    "\n",
    "    Returns:\n",
    "    - dict with keys 'raw_xml_path' and 'normalized_json_path'\n",
    "    \"\"\"\n",
    "    storage_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    paths = {}\n",
    "\n",
    "    if xml_data is not None:\n",
    "        raw_path = storage_dir / \"pubmed_raw.xml\"\n",
    "        with open(raw_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(xml_data)\n",
    "        paths[\"raw_xml_path\"] = raw_path\n",
    "\n",
    "    normalized_path = storage_dir / \"pubmed_normalized.json\"\n",
    "    with open(normalized_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(normalized_docs, f, indent=2)\n",
    "    paths[\"normalized_json_path\"] = normalized_path\n",
    "\n",
    "    return paths\n"
   ],
   "metadata": {
    "id": "xRrrC7AUIZWM"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Parallel Esearch executor\n",
    "Performs parallel exectution on each of the seeds to search the related items."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Reuse your existing REQUEST_DELAY constant and pubmed_search function\n",
    "# REQUEST_DELAY = 0.34  # seconds (NCBI rate limit safety)\n",
    "# def pubmed_search(query: str, retstart=0, retmax=20): ...\n",
    "\n",
    "# --- Rate limiter (global across threads) ---\n",
    "_rate_lock = threading.Lock()\n",
    "_rate_state = {\"last_request_ts\": 0.0}\n",
    "\n",
    "def _rate_limited_request(callable_fn, *args, **kwargs):\n",
    "    \"\"\"Call callable_fn(*args, **kwargs) while respecting REQUEST_DELAY globally.\"\"\"\n",
    "    with _rate_lock:\n",
    "        now = time.time()\n",
    "        elapsed = now - _rate_state[\"last_request_ts\"]\n",
    "        if elapsed < REQUEST_DELAY:\n",
    "            time.sleep(REQUEST_DELAY - elapsed)\n",
    "        # do the call\n",
    "        result = callable_fn(*args, **kwargs)\n",
    "        _rate_state[\"last_request_ts\"] = time.time()\n",
    "    return result\n",
    "\n",
    "# --- Simple stats printer for search ---\n",
    "class SearchStats:\n",
    "    def __init__(self, total_seeds: int):\n",
    "        self.start = time.time()\n",
    "        self.total_seeds = total_seeds\n",
    "        self.seeds_done = 0\n",
    "        self.terms_searched = 0\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def log_seed(self, n_terms: int):\n",
    "        with self.lock:\n",
    "            self.seeds_done += 1\n",
    "            self.terms_searched += n_terms\n",
    "            # print each seed progress and periodic summary\n",
    "            elapsed = time.time() - self.start\n",
    "            rate = self.terms_searched / max(elapsed, 1)\n",
    "            print(\n",
    "                f\"[SEARCH STATS] seeds={self.seeds_done}/{self.total_seeds} | \"\n",
    "                f\"terms_searched={self.terms_searched} | \"\n",
    "                f\"{rate:.2f} terms/sec | \"\n",
    "                f\"{elapsed:.1f}s elapsed\"\n",
    "            )\n",
    "\n",
    "# --- Query builder (MeSH-aware pattern B) ---\n",
    "def build_mesh_aware_query(term: str) -> str:\n",
    "    \"\"\"\n",
    "    Build MeSH-aware query for a single term:\n",
    "      (\"term\"[MeSH Terms] OR \"term\"[Title/Abstract])\n",
    "    \"\"\"\n",
    "    safe = term.replace('\"', '')  # naive sanitization: drop quotes in terms\n",
    "    return f'(\"{safe}\"[MeSH Terms] OR \"{safe}\"[Title/Abstract])'\n",
    "\n",
    "# --- Worker that runs searches for a single seed ---\n",
    "def _search_seed_worker(\n",
    "    seed: Dict[str, Any],\n",
    "    top_k_terms: int = 3,\n",
    "    retmax: int = 200,\n",
    "    stats: Optional[SearchStats] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    For a single seed (dict), run pubmed_search for each preferred_search_term (top_k_terms)\n",
    "    and append entries to seed['pubmed_search_log'].\n",
    "    Returns the updated seed (mutates in-place).\n",
    "    \"\"\"\n",
    "    seed.setdefault(\"preferred_search_terms\", [])\n",
    "    seed.setdefault(\"pubmed_search_log\", [])\n",
    "\n",
    "    terms = seed.get(\"preferred_search_terms\", [])[:top_k_terms]\n",
    "    if not terms:\n",
    "        # fallback: try keyword_candidates top terms\n",
    "        term_candidates = [c.get(\"term\") for c in seed.get(\"keyword_candidates\", []) if c.get(\"term\")]\n",
    "        terms = term_candidates[:top_k_terms]\n",
    "\n",
    "    for term in terms:\n",
    "        query = build_mesh_aware_query(term)\n",
    "        try:\n",
    "            resp = _rate_limited_request(pubmed_search, query, 0, retmax)\n",
    "        except Exception as e:\n",
    "            # log failure for this term\n",
    "            entry = {\n",
    "                \"term\": term,\n",
    "                \"query\": query,\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "                \"pmid_count\": 0,\n",
    "                \"pmids\": [],\n",
    "                \"retrieved_at\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())\n",
    "            }\n",
    "            seed[\"pubmed_search_log\"].append(entry)\n",
    "            continue\n",
    "\n",
    "        esr = resp.get(\"esearchresult\", {}) if isinstance(resp, dict) else {}\n",
    "        idlist = esr.get(\"idlist\", []) or []\n",
    "        count = int(esr.get(\"count\", \"0\") or 0)\n",
    "\n",
    "        entry = {\n",
    "            \"term\": term,\n",
    "            \"query\": query,\n",
    "            \"success\": True,\n",
    "            \"pmid_count\": count,\n",
    "            \"pmids\": idlist,\n",
    "            \"retmax_requested\": retmax,\n",
    "            \"retrieved_at\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())\n",
    "        }\n",
    "        seed[\"pubmed_search_log\"].append(entry)\n",
    "\n",
    "    # update stats and return\n",
    "    if stats:\n",
    "        stats.log_seed(len(terms))\n",
    "    return seed\n",
    "\n",
    "# --- Top-level function: load seeds from directory (optional) ---\n",
    "def load_seed_files_from_dir(seeds_dir: Union[str, Path]) -> List[Dict[str, Any]]:\n",
    "    seeds_dir = Path(seeds_dir)\n",
    "    seeds = []\n",
    "    for p in seeds_dir.glob(\"*.json\"):\n",
    "        try:\n",
    "            with open(p, \"r\", encoding=\"utf-8\") as fh:\n",
    "                data = json.load(fh)\n",
    "            # attach a helper path so we can save the file back later if desired\n",
    "            if isinstance(data, dict):\n",
    "                data.setdefault(\"_seed_file_path\", str(p))\n",
    "                seeds.append(data)\n",
    "            elif isinstance(data, list):\n",
    "                # if file contains list of seeds, flatten\n",
    "                for s in data:\n",
    "                    if isinstance(s, dict):\n",
    "                        s.setdefault(\"_seed_file_path\", str(p))\n",
    "                        seeds.append(s)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] failed to load seed file {p}: {e}\")\n",
    "    return seeds\n",
    "\n",
    "# --- Save updated seed back to its original file (atomic-ish) ---\n",
    "def persist_seed_to_file(seed: Dict[str, Any], overwrite_file: bool = True):\n",
    "    path = seed.get(\"_seed_file_path\")\n",
    "    if not path:\n",
    "        return\n",
    "    try:\n",
    "        p = Path(path)\n",
    "        # If file originally contained a single seed (dict), overwrite.\n",
    "        # If file contained many seeds (list), caller must handle separately.\n",
    "        with open(p, \"w\", encoding=\"utf-8\") as fh:\n",
    "            json.dump(seed, fh, indent=2, ensure_ascii=False)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] failed to persist seed to {path}: {e}\")\n",
    "\n",
    "# --- The parallel search executor ---\n",
    "def pubmed_search_seeds(\n",
    "    seeds: List[Dict[str, Any]],\n",
    "    top_k_terms: int = 3,\n",
    "    retmax: int = 200,\n",
    "    max_workers: int = 8,\n",
    "    persist: bool = True\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Run pubmed searches for many seeds in parallel, using MeSH-aware queries.\n",
    "    - seeds: list of seed dicts (can include _seed_file_path to persist back)\n",
    "    - top_k_terms: how many preferred_search_terms per seed to query\n",
    "    - retmax: retmax passed to pubmed_search per query\n",
    "    - max_workers: thread pool size\n",
    "    - persist: whether to write back updated seed jsons when _seed_file_path present\n",
    "    Returns: list of updated seeds\n",
    "    \"\"\"\n",
    "    stats = SearchStats(total_seeds=len(seeds))\n",
    "    updated = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futures = {ex.submit(_search_seed_worker, seed, top_k_terms, retmax, stats): seed for seed in seeds}\n",
    "        for fut in as_completed(futures):\n",
    "            seed = futures[fut]\n",
    "            try:\n",
    "                updated_seed = fut.result()\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] seed {seed.get('seed_id')} worker failed: {e}\")\n",
    "                # still append original seed\n",
    "                updated.append(seed)\n",
    "                continue\n",
    "\n",
    "            # optionally persist updated seed to its file\n",
    "            if persist and updated_seed.get(\"_seed_file_path\"):\n",
    "                persist_seed_to_file(updated_seed)\n",
    "\n",
    "            # print brief per-seed summary\n",
    "            last_log = updated_seed.get(\"pubmed_search_log\", [])[-1] if updated_seed.get(\"pubmed_search_log\") else None\n",
    "            if last_log:\n",
    "                print(f\"[DONE] {updated_seed.get('seed_id')} — last_term='{last_log.get('term')}' count={last_log.get('pmid_count')}\")\n",
    "            else:\n",
    "                print(f\"[DONE] {updated_seed.get('seed_id')} — no terms searched\")\n",
    "\n",
    "            updated.append(updated_seed)\n",
    "\n",
    "    return updated"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Usage:"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aW7zXNU8EbPS",
    "outputId": "f6823918-15d9-48b4-b2ed-c8b788a0286c"
   },
   "cell_type": "code",
   "source": [
    "# 1) load seeds from directory (optional) - adjust path to your seeds dir\n",
    "seeds = load_seed_files_from_dir(SEED_DIR)\n",
    "\n",
    "# 2) run searches (multi-threaded)\n",
    "updated_seeds = pubmed_search_seeds(seeds, top_k_terms=3, retmax=200, max_workers=8, persist=True)\n",
    "\n",
    "# After this each seed dict will have a 'pubmed_search_log' list appended.\n",
    "\n",
    "\n",
    "# search_resp = pubmed_search(\"risk of coughing\", retmax=5)\n",
    "# search_resp"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}
